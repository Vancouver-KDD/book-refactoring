#### Batch Processing Concepts
- Batch processing is a method of **reading a set of input files** and **generating a set of output files** from them.
- The output is **derived data**, which can be regenerated by rerunning the batch process as needed.
- This method is used to build **search indexes, recommendation systems, and analytics systems**, among others.
- However, batch processing operates on the assumption that **the input data is finite and the entire dataset can be read**.

#### Infinite Data and Stream Processing
- In reality, much data **arrives incrementally over time**, so it is **not finite**.
- Users continuously generate data, meaning that the dataset is never "complete."
- Batch processing addresses this issue by dividing the data into **fixed time intervals** for processing. For example, processing one day's worth of data at a time.
- However, with daily batch processing, **there can be a delay** before changes in the input data are reflected.

#### The Basic Idea of Stream Processing
- Stream processing handles **real-time data** by **processing the data as it arrives**.
- A stream is a sequence of data that is provided gradually over time, often containing **real-time events**.
- Since streaming data is infinite, unlike batch processing, which waits for fixed-size datasets, stream processing **handles data in real time**.

## Transmitting Event Streams

In batch processing, the **input and output** are given as files, while in streaming, **events** are the equivalent concept. An event is an **immutable object** that represents something that occurred at a specific point in time and is stored along with a **timestamp**, which can be processed by multiple consumers.

In batch processing, once files are generated, various jobs can read them. In streaming, events are grouped into **topics** or **streams**, and once an event is produced, it can be processed by multiple consumers.

Polling for new events is inefficient, so it’s better to notify consumers when new events occur. Traditional **databases** do not handle real-time notifications well, so specialized tools have been developed for **event notification**.

### Messaging Systems

A messaging system delivers events from producers to consumers and can scale to allow **multiple producers and consumers to share the same topic**. It typically follows a **publish/subscribe** model, and different messaging systems manage messages in various ways.

1. **What if producers send messages too quickly?**
   - Options include **dropping**, **buffering**, or **backpressure** (flow control).
   - Backpressure works like a Unix pipe or TCP, **blocking the sender until the receiver can process the data**.

2. **What if nodes crash or go offline?**
   - To ensure durability, messages can be **written to disk or replicated**.
   - Higher throughput can be achieved by allowing message loss, but this depends on **reliability requirements**.

#### Direct Messaging from Producers to Consumers
- Systems like **UDP multicast**, **ZeroMQ**, and **StatsD** facilitate direct communication between producers and consumers. Though message loss can occur, protocols exist to detect and recover from this.
- If the network service is exposed, producers can directly send messages using **HTTP** or **RPC requests**.
- Such systems typically assume **online operation**, but if offline, messages may be lost.

#### Message Brokers
- **Message brokers** mediate between producers and consumers, centralizing the storage and management of messages.
- Brokers ensure **durability** and allow **slow consumers** to process messages without missing any.
- Message brokers operate **asynchronously**, allowing producers to send messages even if consumers haven't yet processed them.

#### Message Brokers Compared to Databases
- Databases retain data until it’s explicitly deleted, while **message brokers** generally **automatically delete** messages after they have been delivered to consumers.
- Message brokers support **topic-based subscription** and excel in **change notifications**.

#### Multiple Consumers
- **Load Balancing**: Each message is delivered to **only one consumer**, distributing the workload.
- **Fan-out**: Each message is delivered to **all consumers**, allowing independent processing by each.

#### Acknowledgments and Redelivery
- Messages are removed from the broker's queue only after being **acknowledged** by the consumer. If a failure occurs, the message is **redelivered** to another consumer.
- This can result in **reordering** of messages. Separate queues can be used to prevent reordering.

In summary, **messaging systems** play a critical role in balancing **reliability** and **performance** in stream processing, requiring careful design and strategy based on the situation.

### Partitioned Logs

#### Using Logs for Message Storage
Log-based message brokers store messages in an **append-only** log format, and consumers read them sequentially. This allows **data to remain intact** and accessible to multiple consumers. **Partitioning** enables the log to be distributed across multiple machines, achieving **high throughput**.

#### Logs Compared to Traditional Messaging
Log-based systems easily support **fan-out messaging**, allowing each consumer to read the log independently. However, the number of partitions is limited, so **preserving message order within a partition** is necessary, and **bottlenecks** can occur. **JMS/AMQP-style brokers** are better suited for fine-grained parallelism at the message level, while **log-based systems** excel in scenarios where message order is crucial.

#### Consumer Offsets
In log-based systems, consumers track which messages have been processed based on **offsets**. By recording offsets, it’s easier to avoid duplicate processing and handle restarts after failures.

#### Disk Space Usage
Logs manage disk space by deleting old messages or **archiving** them. Typically, disks store **days to weeks** of messages while maintaining **continuous throughput**. Slow consumers are warned but don’t affect others.

#### When Consumers Cannot Keep Up with Producers
Log-based systems offer a large buffer using **disk space**, but when the buffer is full, old messages are deleted. Slow consumers are warned and adjustments are made to avoid missing messages.

#### Replaying Old Messages
Log-based brokers support **non-destructive message processing**. Consumers can adjust **offsets** to re-read and reprocess past messages, facilitating **repeatable data processing** similar to batch processing, aiding in experimentation and error recovery.

Ultimately, log-based message brokers are **reliable**, **scalable**, and **flexible** systems that are well-suited for **high throughput** and **data integration**.

## Databases and Streams

Log-based message brokers are an example of applying **database ideas to messaging**. Conversely, **messaging and stream ideas** can be applied to databases. An event is a record of something that happened at a specific point in time, and this applies not only to user behavior but also to **data changes written to a database**.

The **replication log** is a stream of database write events that allows replicas to stay in sync. According to the **State Machine Replication** principle, all replicas will converge to the same final state if they process the same events in the same order.

This section explores combining the ideas of **event streams and databases** to solve problems in **heterogeneous data systems**.

### Keeping Systems in Sync

A single system cannot meet all the data storage, query, and processing needs, so complex applications combine various technologies. For example, they might use OLTP databases, caches, search indexes, data warehouses, and more. Data must stay synchronized between these systems, often using batch processes or a "dual writes" approach.

However, **dual writes** can cause synchronization issues. For example, if two systems write different values simultaneously, a **race condition** may occur, resulting in discrepancies between the database and the search index. Additionally, if one write fails, fault tolerance issues arise, leading to persistent inconsistencies.

A single-leader approach can mitigate these issues, but if each system has its own independent leader, conflicts can occur. Having **one leader** to determine all write order is a better solution.

### Change Data Capture (CDC)

**Change Data Capture (CDC)** is a technique that tracks **all data changes** occurring in a database and replicates them to **other systems**. CDC provides a **stream of real-time data changes** from the database, which can be used to update derived data systems like search indexes, caches, or data warehouses.

#### Implementing Change Data Capture
- CDC is implemented using a database's **change log**, which is used to maintain a replica of the database.
- A log-based message broker can **stream database changes in order** to support CDC.
- CDC can be implemented using **trigger-based** or **log parsing-based** methods, with the latter being more robust and performant.

#### Initial Snapshot
- When setting up a new system, a **full copy** of the database is needed. This is done by taking an **initial snapshot** and then applying change logs in order.
- The snapshot must be coordinated with a **specific point in the change log**, and some CDC tools automate this process.

#### Log Compaction
- **Log compaction** retains only the most recent value for each key and deletes older values, saving **disk space**.
- This allows CDC systems to efficiently replicate database contents using **less storage space**.

#### API Support for Change Streams
- More databases are offering APIs to support **change streams**. For example, **RethinkDB, Firebase, CouchDB**, and others provide the ability to subscribe to database changes.
- **Kafka Connect** integrates with CDC tools, enabling change data from various databases to be streamed through **Kafka** and used in derived data systems or stream processing systems.

In summary, CDC ensures **data consistency** and **reliability** across multiple systems while minimizing the issues of replication delays.

### Event Sourcing

**Event sourcing** records all **state changes** of an application

 as an **event log**, similar to CDC but with differences in the level of **abstraction**.

- In **CDC**, database changes are automatically recorded, while in **event sourcing**, application logic explicitly writes to an event log.
- Events are immutable and represent facts about what happened within the application, with updates or deletions being restricted.

#### Deriving Current State from the Event Log
- With event sourcing, the **current state** of the application can be reconstructed by processing the **event log**.
- While the log records all changes, users are typically interested in the **current state**, requiring processing logic to derive the **final state** from the events.
- Unlike CDC, event sourcing **retains the entire event history**, so log compaction is handled differently.

#### Commands and Events
- In event sourcing, **commands** and **events** are distinct concepts.
- A user’s request is a command, but once the command is successfully processed, it becomes an immutable event.
- Events are treated as **facts** within the system and, once generated, cannot be altered. All validation occurs **before** they become events.

This approach efficiently manages application state and allows for easy tracking of all changes.

### State, Streams, and Immutability

In Chapter 10, we discussed how **batch processing** benefits from the **immutability of input files**, ensuring safe handling of data. Immutability plays a similarly important role in **event sourcing** and **change data capture (CDC)**.

- **Databases store the current state** of an application, and state is inherently **mutable**. Changes in state are the result of **immutable event logs** that record what happened.
- The state and event logs are not in conflict; rather, the state is derived from the events that occur over time.

#### Advantages of Immutable Events
- An **immutable event log** clearly records what happened and makes recovery easier.
- This is particularly useful in systems like **accounting**, where errors cannot be overwritten but can be corrected instead.

#### Deriving Several Views from the Same Event Log
- Immutable event logs allow for the creation of **multiple read views**, enhancing **flexibility**. This makes it easier to introduce **new features** or display data in different ways.
- **CQRS** (Command Query Responsibility Segregation) separates writing from reading, optimizing **data writes and reads** independently.

#### Concurrency Control
- Event sourcing and CDC operate **asynchronously**, so user-written data may **not yet be reflected in the read view**.
- This can be managed with **synchronous updates** or by simplifying **concurrency control** by limiting write operations to **a single location**.

#### Limitations of Immutability
- While an immutable data model aids in recovery and auditing, **storing all data permanently** can be inefficient with large datasets.
- Legal or administrative requirements may necessitate **data deletion**, which requires balancing the benefits of immutability with the need for deletion.

In conclusion, **immutability** helps safely manage data, track errors, and easily recover data, but it also introduces challenges in deletion and management.

## Processing Streams

1. **Data storage**: Event data can be stored in systems like databases, caches, or search indexes, allowing other clients to query the data. This is a good way to keep other parts of the system synchronized with the database.
2. **Push to users**: Events can be streamed to **real-time dashboards** or used to send **email notifications**, providing real-time information to users.
3. **Generating derived streams**: Processing input streams to generate new output streams. These processes are called **operators** or **jobs** and function similarly to Unix processes or MapReduce jobs.

The key difference is that a stream is an **unending dataset**. This affects sorting and fault-tolerance mechanisms, and unlike batch jobs, it’s impractical to restart a stream processing job that has been running for years from the beginning.

### Uses of Stream Processing

**Stream processing** is applied in various domains, initially used for **monitoring** and **alerting systems** but now expanded to many other applications. Let’s briefly compare different use cases.

#### Complex Event Processing (CEP)
- **CEP** focuses on detecting **specific event patterns**.
- It uses **state machines** to continuously monitor event streams and emits a complex event when a pattern is detected.
- Tools like **Esper** and **TIBCO StreamBase** are used for CEP implementation.

#### Stream Analytics
- **Stream analytics** emphasizes **aggregation** and **statistical analysis** rather than event patterns.
- Example: Counting event occurrences, calculating moving averages, detecting trends, etc.
- Open-source tools like **Apache Storm** and **Flink** are commonly used.

#### Maintaining Materialized Views
- Using **change streams from databases** to update **caches**, **search indexes**, or **data warehouses**, maintaining **materialized views** in the process.
- Tools like **Kafka Streams** and **Samza** support this role.

#### Search on Streams
- **Stream search** is a method for finding specific events based on **complex criteria**.
- Example: A **media monitoring service** that finds news mentioning a particular topic.
- **Elasticsearch's percolator** feature implements such search functionality.

#### Message Passing and RPC
- **Message passing systems** and **RPC** manage service-to-service communication, though they are not typically considered **stream processing**.
- For example, **Actor frameworks** manage **concurrency and distributed execution**, while **stream processing** primarily focuses on **data management**.

Stream processing is used across various fields, such as **analytics**, **search**, and **state maintenance**, and can be implemented using a range of tools and frameworks.

### Reasoning About Time

Handling time in stream processing can be complex. Even simple tasks like calculating a "5-minute average" become challenging due to different ways of managing time.

#### Event Time vs. Processing Time
Event time and processing time can differ. Events may be delayed or arrive out of order, leading to inaccuracies if processing time is used. To achieve consistent results, processing must be based on event time.

#### Knowing When You're Ready
It's difficult to know when all events for a given window have arrived. Events can be delayed, resulting in **straggler events**. Possible strategies include ignoring stragglers or providing corrected results after their arrival.

#### Whose Clock Are You Using, Anyway?
Event timestamps may vary depending on whether they come from the device’s local clock or the server's clock. To correct clock discrepancies, three timestamps (event occurrence time, transmission time, and reception time) can be recorded.

#### Types of Windows
- **Tumbling Window**: Fixed-length window in which each event belongs to only one window.
- **Hopping Window**: Fixed-length windows that overlap.
- **Sliding Window**: Includes all events within a certain interval.
- **Session Window**: Has no fixed duration and ends based on user activity.

In stream processing, it's essential to treat **event time and processing time separately**, using various window strategies to group and analyze events.

### Stream Joins

**Joins** are essential in stream processing. There are three types of stream joins: **stream-stream join**, **stream-table join**, and **table-table join**. Each type of join depends heavily on time.

#### Stream-Stream Join (Window Join)
- Joins **two streams**, for example, by joining web search events with click events to calculate click-through rates.
- Events are joined when they occur within a **time window**.
- The stream processor **maintains state**, storing all events within a given period and performing the join when matching events occur.

#### Stream-Table Join (Stream Enrichment)
- Joins a **stream with a table**, adding extra information to the stream event.
- Example: Enriching user activity events with user profile data.
- Since **database changes** must also be reflected, the **changelog** of the profile database is joined with the activity event stream to maintain up-to-date information.

#### Table-Table Join (Materialized View Maintenance)
- Joins **two tables** and is mainly used to **maintain materialized views**.
- Example: Twitter's timeline cache is maintained by joining follow relationships and tweets.
- Streams of tweets and follow relationship events are used to update the timeline in real time.

#### Time Dependence of Joins
- The **time dependence of joins** is crucial. The outcome may vary depending on when the event occurs.
- **Join order** may lead to non-deterministic results, so **event order and timestamps** must be handled carefully.
- For example, when tax rates or product information change over time, it’s important to ensure the join reflects the correct state at the right time.

Stream joins are **time-sensitive** and must be handled with care to ensure correct results based on the timing and sequence of events.

### Fault Tolerance

**Fault tolerance** is also critical in stream processing. While batch jobs can easily be restarted, in a never-ending stream, more complex problems arise.

#### Microbatching and Checkpointing
- **Microbatching** divides the stream into small batches for processing, with each batch acting like a tumbling window.
- **Checkpointing** periodically saves the stream's state so that processing can resume from the most recent checkpoint if a failure occurs. **Apache Flink** uses this approach.

#### Atomic Commit Revisited
- **Atomic commit** must ensure that all processing results are applied only when the operation completes successfully. This creates the appearance of exactly-once processing.
- Rather than using distributed transactions, state changes and messaging can be managed within the stream framework.

#### Idempotence
- **Idempotent operations** have the same effect when executed multiple times. For example, setting a value multiple times results in the same state.
- Non-idempotent operations can be made idempot

ent by adding metadata, ensuring that the same message isn’t processed multiple times after a restart.

#### Rebuilding State After a Failure
- **State recovery** is a key aspect of stream processing. States may be stored locally or periodically replicated.
- If the state is an aggregation over a short window, the input events can be replayed to recover the state quickly.

In summary, achieving **fault tolerance** in stream processing requires careful attention to **state management**, **idempotence**, and recovery through **checkpointing**, depending on the system's performance characteristics and requirements.